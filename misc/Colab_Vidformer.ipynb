{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gI19RpsKVwl"
   },
   "outputs": [],
   "source": [
    "!pip3 install vidformer supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcayluwkjWF2"
   },
   "source": [
    "# [Vidformer](https://github.com/ixlab/vidformer): Video Data Transformation\n",
    "\n",
    "Vidformer uses a `cv2`-compatibility layer allowing `import vidformer.cv2 as cv2` conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfvXyItHKbOS"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import vidformer as vf\n",
    "import vidformer.cv2 as cv2\n",
    "import supervision as sv\n",
    "import vidformer.supervision as vf_sv\n",
    "\n",
    "# Use the api.vidformer.org guest account\n",
    "# The guest account has few permissions (can't access other videos) and low limits\n",
    "# To get around this:\n",
    "#     1) Ask for a regular account\n",
    "#     2) Self-host\n",
    "server = vf.Server(\"https://api.vidformer.org\", api_key=\"VF_GUEST\", vod_only=True)\n",
    "cv2.set_server(server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "579M7jwiKgu2",
    "outputId": "6d474ac3-ca50-48e3-d56b-9d026925bf5e"
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"https://f.dominik.win/vf-sample-media/tos_720p.mp4\")\n",
    "assert cap.isOpened()\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "out = cv2.VideoWriter(None, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "\n",
    "# Play the video in the notebook cell (outside jupyter add method=\"link\")\n",
    "# This will say \"Waiting\" until you fill in the content by running the next cell\n",
    "cv2.vidplay(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWHooZspKnhw"
   },
   "outputs": [],
   "source": [
    "radius = 100\n",
    "center_x, center_y = 300, 300\n",
    "speed = 2 * math.pi / 100\n",
    "i = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    angle = i * speed\n",
    "    text_x = int(center_x + radius * math.cos(angle))\n",
    "    text_y = int(center_y + radius * math.sin(angle))\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        \"Hello, world!\",\n",
    "        (text_x, text_y),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 255, 0),\n",
    "        2,\n",
    "        cv2.LINE_AA,\n",
    "    )\n",
    "    out.write(frame)\n",
    "    i += 1\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLvbF0ryjdZx"
   },
   "source": [
    "## Vidformer for CV Annotation with supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "51o-2E-2lnoo",
    "outputId": "5ca8d6b9-5a73-45c0-a4ab-1daa69629f2a"
   },
   "outputs": [],
   "source": [
    "# # Run Yolov8m on the video\n",
    "# import cv2 as ocv_cv2\n",
    "# import supervision as sv\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# model = YOLO(\"yolov8m.pt\")\n",
    "\n",
    "# ocv_cap = ocv_cv2.VideoCapture(\n",
    "#     \"https://f.dominik.win/vf-sample-media/tos_720p.mp4\"\n",
    "# )\n",
    "# assert ocv_cap.isOpened()\n",
    "\n",
    "# detections = []\n",
    "# while True:\n",
    "#   ret, frame = ocv_cap.read()\n",
    "#   if not ret:\n",
    "#     break\n",
    "#   detections.append(sv.Detections.from_ultralytics(model(frame)[0]))\n",
    "\n",
    "\n",
    "# Or just load pre-computed detections instead to save some time\n",
    "import pickle\n",
    "import urllib\n",
    "\n",
    "with urllib.request.urlopen(\n",
    "    \"https://f.dominik.win/vf-sample-media/tos_720p-yolov8l-detections.pkl\"\n",
    ") as response:\n",
    "    detections = pickle.load(response)\n",
    "\n",
    "out = cv2.VideoWriter(None, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "cv2.vidplay(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Il7LiPRJ6KHg"
   },
   "outputs": [],
   "source": [
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "box_anot = vf_sv.BoxAnnotator()\n",
    "label_anot = vf_sv.LabelAnnotator()\n",
    "i = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    det = detections[i]\n",
    "    det = det[det.confidence > 0.5]\n",
    "\n",
    "    labels = [\n",
    "        f\"{class_name} {confidence:.2f}\"\n",
    "        for class_name, confidence in zip(det[\"class_name\"], det.confidence)\n",
    "    ]\n",
    "    frame = box_anot.annotate(frame.copy(), det)\n",
    "    frame = label_anot.annotate(frame.copy(), det, labels)\n",
    "\n",
    "    out.write(frame)\n",
    "    i += 1\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6ihcQXAk_oE"
   },
   "source": [
    "## Going beyond simple stream annotation\n",
    "\n",
    "Vidformer doesn't just hard-code the simple case of video stream annotation, it's a generalized video transformation system.\n",
    "It uses a novel decoding system to efficiently access source frames:\n",
    "\n",
    "- **Use frames in any order:** Sped up, reversed, repeated access, even randomly shuffled if you have the compute to support it. Vidformer finds efficient access plans so you don't have to.\n",
    "- **Use frames from multiple sources:** Create transformed videos from one video or millions of source videos.\n",
    "- **Combine multiple frames together:** Compose frames side-by-side, in a grid, or anything else you can think of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "CKhopAWBlErR",
    "outputId": "1810f9db-5a68-49f9-8ca6-f63915482dc8"
   },
   "outputs": [],
   "source": [
    "out = cv2.VideoWriter(None, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "cv2.vidplay(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYG7jH55lK1G"
   },
   "outputs": [],
   "source": [
    "def frame_n(n):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, n)\n",
    "    ret, frame = cap.read()\n",
    "    assert ret\n",
    "    return frame\n",
    "\n",
    "\n",
    "half_size = (height // 2, width // 2)\n",
    "for i in range(frame_count):\n",
    "    frame = cv2.zeros((height, width, 3))\n",
    "\n",
    "    f_sped_up = frame_n(i * 2 % frame_count)\n",
    "    f_sped_up = cv2.resize(f_sped_up, (half_size[1], half_size[0]))\n",
    "    y_offset, x_offset = (height - half_size[0]) // 2, 0\n",
    "    frame[y_offset : y_offset + half_size[0], x_offset : x_offset + half_size[1]] = (\n",
    "        f_sped_up\n",
    "    )\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        \"Sped up 2x:\",\n",
    "        (x_offset + 10, y_offset - 10),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (255, 255, 255),\n",
    "        2,\n",
    "        cv2.LINE_AA,\n",
    "    )\n",
    "\n",
    "    f_reversed = frame_n(frame_count - i - 1)\n",
    "    f_reversed = cv2.resize(f_reversed, (half_size[1], half_size[0]))\n",
    "    y_offset, x_offset = (height - half_size[0]) // 2, width // 2\n",
    "    frame[y_offset : y_offset + half_size[0], x_offset : x_offset + half_size[1]] = (\n",
    "        f_reversed\n",
    "    )\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        \"Reversed:\",\n",
    "        (x_offset + 10, y_offset - 10),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (255, 255, 255),\n",
    "        2,\n",
    "        cv2.LINE_AA,\n",
    "    )\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data in Videos: Object Masks\n",
    "\n",
    "Object masks can be large, often 10x larger than the underlying video when decompressed. Vidformer is good at accessing frames out of order so you can store data in video files with lossless codecs (like FFV1). For example, each object mask can be stored as a seperate frame and stitched together during viewing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# from ultralytics import YOLO\n",
    "# import pickle\n",
    "\n",
    "# cap = cv2.VideoCapture(\n",
    "#     \"https://f.dominik.win/vf-sample-media/tos_720p.mp4\"\n",
    "# )\n",
    "# model = YOLO(\"yolov8x-seg.pt\")\n",
    "\n",
    "# msw = vf_sv.MaskStreamWriter(\"tos_720p-yolov8x-seg-masks.mkv\", (1280, 720))\n",
    "# detections = []\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "#     results = model(frame, verbose=False)[0]\n",
    "#     det = sv.Detections.from_ultralytics(results)\n",
    "#     msw.write_detections(det)\n",
    "#     det.mask = None\n",
    "#     detections.append(det)\n",
    "# cap.release()\n",
    "# msw.release()\n",
    "\n",
    "# with open(\"tos_720p-yolov8x-seg-detections.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(detections, f)\n",
    "\n",
    "import urllib\n",
    "import pickle\n",
    "\n",
    "# Load detections with from yolov8x-seg\n",
    "with urllib.request.urlopen(\n",
    "    \"https://f.dominik.win/vf-sample-media/tos_720p-yolov8x-seg-detections.pkl\"\n",
    ") as response:\n",
    "    detections = pickle.load(response)\n",
    "\n",
    "# Open the detection masks compressed into a video file\n",
    "mask_cap = cv2.VideoCapture(\n",
    "    \"https://f.dominik.win/vf-sample-media/tos_720p-yolov8x-seg-masks.mkv\"\n",
    ")\n",
    "\n",
    "out = cv2.VideoWriter(None, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "cv2.vidplay(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "mask_anot = vf_sv.MaskAnnotator()\n",
    "label_anot = vf_sv.LabelAnnotator(text_position=sv.Position.CENTER)\n",
    "\n",
    "i = 0\n",
    "mask_i = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    det = detections[i]\n",
    "    vf_sv.populate_mask(det, mask_cap, mask_i)\n",
    "    mask_i += len(det)\n",
    "\n",
    "    labels = [\n",
    "        f\"{class_name} {confidence:.2f}\"\n",
    "        for class_name, confidence in zip(det[\"class_name\"], det.confidence)\n",
    "    ]\n",
    "    frame = label_anot.annotate(frame.copy(), det, labels)\n",
    "    frame = mask_anot.annotate(frame.copy(), det)\n",
    "    det.mask = None\n",
    "\n",
    "    out.write(frame)\n",
    "    i += 1\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Search Engine\n",
    "\n",
    "Here we search for the word \"Houston\" from many hours of Apollo 11 videos. It also has some niceties, like rendering subtitles. Subtitle files were created with whisper turbo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import requests\n",
    "\n",
    "BASE = \"https://f.dominik.win/vf-sample-media/apollo-11-mission/Apollo 11 {xx}\"\n",
    "PARTS = [f\"{i:02d}\" for i in range(1, 24)]\n",
    "\n",
    "QUERY = \"Houston\"  # Search term\n",
    "\n",
    "PAD_BEFORE = 0.25\n",
    "PAD_AFTER = 0.35\n",
    "\n",
    "\n",
    "def srt_to_sec(ts):\n",
    "    hh, mm, ss, ms = re.match(r\"(\\d+):(\\d+):(\\d+)[,\\.](\\d+)\", ts.strip()).groups()\n",
    "    return int(hh) * 3600 + int(mm) * 60 + int(ss) + int(ms) / 1000.0\n",
    "\n",
    "\n",
    "def parse_srt(text):\n",
    "    cues = []\n",
    "    text = text.strip().replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    for block in re.split(r\"\\n\\s*\\n\", text):\n",
    "        lines = [x.strip() for x in block.split(\"\\n\") if x.strip()]\n",
    "        if not lines:\n",
    "            continue\n",
    "        if lines[0].isdigit():\n",
    "            lines = lines[1:]\n",
    "        if not lines:\n",
    "            continue\n",
    "        m = re.match(r\"(.*?)\\s*-->\\s*(.*?)(?:\\s+.*)?$\", lines[0])\n",
    "        if not m:\n",
    "            continue\n",
    "        s = srt_to_sec(m.group(1))\n",
    "        e = srt_to_sec(m.group(2))\n",
    "        t = \"\\n\".join(lines[1:]).strip()\n",
    "        if t:\n",
    "            cues.append((s, e, t))\n",
    "    return cues\n",
    "\n",
    "\n",
    "def find_intervals(cues, q, pad_before, pad_after):\n",
    "    q = q.lower()\n",
    "    hits = []\n",
    "    for s, e, t in cues:\n",
    "        if q in t.lower():\n",
    "            hits.append((max(0, s - pad_before), e + pad_after))\n",
    "    hits.sort()\n",
    "    merged = []\n",
    "    for s, e in hits:\n",
    "        if not merged or s > merged[-1][1]:\n",
    "            merged.append([s, e])\n",
    "        else:\n",
    "            merged[-1][1] = max(merged[-1][1], e)\n",
    "    return [(s, e) for s, e in merged]\n",
    "\n",
    "\n",
    "def active_text(cues, t):\n",
    "    # include overlapping cues, unique them, preserve order\n",
    "    out = [txt for s, e, txt in cues if s <= t <= e]\n",
    "    seen, uniq = set(), []\n",
    "    for x in out:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            uniq.append(x)\n",
    "    return \"\\n\".join(uniq).strip()\n",
    "\n",
    "\n",
    "def draw_subs(frame, text):\n",
    "    if not text:\n",
    "        return frame\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # wrap width roughly proportional to resolution\n",
    "    max_chars = int(w / 22)  # ~58 for 1280px wide\n",
    "    max_chars = max(24, min(80, max_chars))\n",
    "\n",
    "    # wrap each original line\n",
    "    lines = []\n",
    "    for ln in text.splitlines():\n",
    "        ln = ln.strip()\n",
    "        if not ln:\n",
    "            continue\n",
    "        while len(ln) > max_chars:\n",
    "            cut = ln.rfind(\" \", 0, max_chars)\n",
    "            if cut == -1:\n",
    "                cut = max_chars\n",
    "            lines.append(ln[:cut].strip())\n",
    "            ln = ln[cut:].strip()\n",
    "        lines.append(ln)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    # scale relative to 720p baseline\n",
    "    scale = 0.9 * (h / 720.0)\n",
    "    scale = max(0.5, min(1.4, scale))\n",
    "\n",
    "    thick = max(1, int(round(2 * (h / 720.0))))\n",
    "    outline = thick + max(2, int(round(4 * (h / 720.0))))\n",
    "\n",
    "    sizes = [cv2.getTextSize(ln, font, scale, thick)[0] for ln in lines]\n",
    "    line_h = max(sz[1] for sz in sizes) + int(10 * (h / 720.0))\n",
    "\n",
    "    bottom_margin = int(60 * (h / 720.0))\n",
    "    y0 = h - bottom_margin - line_h * len(lines)\n",
    "\n",
    "    for i, ln in enumerate(lines):\n",
    "        tw, th = sizes[i]\n",
    "        x = (w - tw) // 2\n",
    "        y = y0 + i * line_h + th\n",
    "\n",
    "        # outline then fill\n",
    "        cv2.putText(frame, ln, (x, y), font, scale, (0, 0, 0), outline, cv2.LINE_AA)\n",
    "        cv2.putText(frame, ln, (x, y), font, scale, (255, 255, 255), thick, cv2.LINE_AA)\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "def draw_overlay(frame, label, t):\n",
    "    h = frame.shape[0]\n",
    "\n",
    "    scale = 0.6 * (h / 720.0)\n",
    "    scale = max(0.4, min(1.0, scale))\n",
    "\n",
    "    thick = max(1, int(round(2 * (h / 720.0))))\n",
    "    outline = thick + 2\n",
    "\n",
    "    hh = int(t // 3600)\n",
    "    mm = int((t % 3600) // 60)\n",
    "    ss = int(t % 60)\n",
    "    ms = int((t - int(t)) * 1000)\n",
    "\n",
    "    time_str = f\"{hh:02d}:{mm:02d}:{ss:02d}.{ms:03d}\"\n",
    "    text = f\"{label}  |  {time_str}\"\n",
    "\n",
    "    x = 20\n",
    "    y = int(40 * (h / 720.0))\n",
    "\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        text,\n",
    "        (x, y),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        scale,\n",
    "        (0, 0, 0),\n",
    "        outline,\n",
    "        cv2.LINE_AA,\n",
    "    )\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        text,\n",
    "        (x, y),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        scale,\n",
    "        (255, 255, 255),\n",
    "        thick,\n",
    "        cv2.LINE_AA,\n",
    "    )\n",
    "\n",
    "\n",
    "first_url = BASE.format(xx=PARTS[0]) + \".mp4\"\n",
    "cap0 = cv2.VideoCapture(first_url)\n",
    "assert cap0.isOpened()\n",
    "\n",
    "fps = cap0.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "w = int(cap0.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap0.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "cap0.release()\n",
    "\n",
    "writer = cv2.VideoWriter(None, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "cv2.vidplay(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xx in PARTS:\n",
    "    base = BASE.format(xx=xx)\n",
    "    vid_url = base + \".mp4\"\n",
    "    sub_url = (base + \".srt\").replace(\" \", \"%20\")\n",
    "\n",
    "    cues = parse_srt(requests.get(sub_url).text)\n",
    "    intervals = find_intervals(cues, QUERY, PAD_BEFORE, PAD_AFTER)\n",
    "\n",
    "    cap = cv2.VideoCapture(vid_url)\n",
    "    if not cap.isOpened():\n",
    "        continue\n",
    "\n",
    "    for a, b in intervals:\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, int(a * 1000))\n",
    "\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "\n",
    "            t = cap.get(cv2.CAP_PROP_POS_FRAMES) / fps\n",
    "            if t > b:\n",
    "                break\n",
    "            if frame.shape[1] != w or frame.shape[0] != h:\n",
    "                frame = cv2.resize(frame, (w, h))\n",
    "            frame = draw_subs(frame, active_text(cues, t))\n",
    "            draw_overlay(frame, f\"Apollo 11 {xx}.mp4\", t)\n",
    "            writer.write(frame)\n",
    "    cap.release()\n",
    "\n",
    "writer.release()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
